{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries: spaCy library, which is used for processing and analyzing text in natural language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a spaCy Object: This initializes a blank spaCy pipeline for the English language (\"en\").\n",
    "A \"blank\" pipeline means it doesn’t have pre-trained models for tasks like Named Entity Recognition (NER) or part-of-speech tagging. It's essentially an empty pipeline that can be used to process text without these advanced features.\n",
    "\n",
    "The doc object contains the processed text and can be further analyzed, such as extracting tokens, named entities, or performing other NLP tasks like parsing and tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"During my studies, I was able to acquire a set of faculties that enabled me to reinforce my skills in multiple areas: a significant knowledge of the fundamentals of Python in order to implement data science techniques and to build machine learning as well as deep learning models. \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is how tokenization is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During\n",
      "my\n",
      "studies\n",
      ",\n",
      "I\n",
      "was\n",
      "able\n",
      "to\n",
      "acquire\n",
      "a\n",
      "set\n",
      "of\n",
      "faculties\n",
      "that\n",
      "enabled\n",
      "me\n",
      "to\n",
      "reinforce\n",
      "my\n",
      "skills\n",
      "in\n",
      "multiple\n",
      "areas\n",
      ":\n",
      "a\n",
      "significant\n",
      "knowledge\n",
      "of\n",
      "the\n",
      "fundamentals\n",
      "of\n",
      "Python\n",
      "in\n",
      "order\n",
      "to\n",
      "implement\n",
      "data\n",
      "science\n",
      "techniques\n",
      "and\n",
      "to\n",
      "build\n",
      "machine\n",
      "learning\n",
      "as\n",
      "well\n",
      "as\n",
      "deep\n",
      "learning\n",
      "models\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access tokens one be one and check some rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "During"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "firsttoken= doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is alphabetic or not\n",
    "firsttoken.is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is__numeric or not\n",
    "firsttoken.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'During'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the text of the token\n",
    "firsttoken.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from a text file, using tokenization and their rules, we can print only token that are considered as an email address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\n',\n",
       " '==================================================\\n',\n",
       " '\\n',\n",
       " 'Name\\tbirth day   \\temail\\n',\n",
       " '-----\\t------------\\t------\\n',\n",
       " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
       " 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n",
       " 'Serena  24 June, 1998   serena@williams.com \\n',\n",
       " 'Joe      1 May, 1997    joe@root.com\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"C:/Users/Wajih/Desktop/Projects Wajih/Exploring SpaCy/students.txt\") as f:\n",
    "    text = f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\n ==================================================\\n \\n Name\\tbirth day   \\temail\\n -----\\t------------\\t------\\n Virat   5 June, 1882    virat@kohli.com\\n Maria\\t12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com \\n Joe      1 May, 1997    joe@root.com\\n \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joe@root.com\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "emails=[]\n",
    "print(doc[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[virat@kohli.com, maria@sharapova.com, serena@williams.com, joe@root.com]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "emails=[]\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token)\n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add special rules for a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'wannabe', 'buying', 'Two', 'apples', 'and', '1', 'banana', 'for', 'Just', '1.5', '$', '!']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I wannabe buying Two apples and 1 banana for Just 1.5$!\")\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'wanna', 'be', 'buying', 'Two', 'apples', 'and', '1', 'banana', 'for', 'Just', '1.5', '$', '!']\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"wannabe\",[{ORTH:\"wanna\"},{ORTH:\"be\"}])\n",
    "\n",
    "doc = nlp(\"I wannabe buying Two apples and 1 banana for Just 1.5$!\")\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In spaCy, the nlp.add_pipe(\"sentencizer\") command adds a sentencizer component to the NLP processing pipeline.\n",
    "--> You can then access the sentences using \"doc.sents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x2a864b0cd50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"sentencizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Joujou wannabe buying Two apples and 1 banana for Just 1.5$!\n",
      "i do not have money\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Joujou wannabe buying Two apples and 1 banana for Just 1.5$! i do not have money\")\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract Urls from a certain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.data.gov/', 'http://www.science.gov/', 'http://data.gov.uk/.', 'http://www3.norc.org/gss+website/', 'http://www.europeansocialsurvey.org/.']\n"
     ]
    }
   ],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n",
    "\n",
    "doc = nlp(text)\n",
    "urls = [token.text for token in doc if token.like_url]\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract amounts of transactions from a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two $', '500 €']\n"
     ]
    }
   ],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "\n",
    "doc = nlp(transactions)\n",
    "a = []\n",
    "for i in range(0,len(doc)-2):\n",
    "    if doc[i].like_num and doc[i+1].is_currency:\n",
    "        a.append(' '.join([str(doc[i]),str(doc[i+1])]))\n",
    "\n",
    "print(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony\n",
      "gave\n",
      "two\n",
      "$\n",
      "to\n",
      "Peter\n",
      ",\n",
      "Bruce\n",
      "gave\n",
      "500\n",
      "€\n",
      "to\n",
      "Steve\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "text = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* spacy.blank():\n",
    "-Creates a black spaCy pipeline for a specific language like english\n",
    "-contains only a tokenizer by default\n",
    "-No pre-trained data or components are included(like POS tagging or named entity recognition)\n",
    "-through this blank pipeline, we can customize it by adding our own componenes like sentencizer\n",
    "* spacy.load(\"en_core_web_sm\"):\n",
    "-loads a pre-trained spaCy language model (english)\n",
    "-includes pre-trained word vectors\n",
    "-can do tokenization, POS tagging, NER, sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads the small English language model (\"en_core_web_sm\") in spaCy for natural language processing tasks.\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a list of the names of the processing pipelines in the loaded spaCy model.\n",
    "Let me explain it more simply:\n",
    "\n",
    "In spaCy, when you load a language model (like nlp = spacy.load(\"en_core_web_sm\")), it comes with a series of steps, or \"components,\" that help process text. These components could include things like:\n",
    "\n",
    "-Tokenizer (splits text into words)\n",
    "-POS Tagger (labels words with their grammatical role, like noun or verb)\n",
    "-Named Entity Recognizer (identifies names of people, places, etc.)\n",
    "The command nlp.pipe_names gives you the names of all these steps in the order spaCy applies them. For example, it might return:\n",
    "\n",
    "\n",
    "The output ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'] represents the components in the spaCy pipeline, listed in the order they are applied to the text:\n",
    "\n",
    "tok2vec: Converts words into word vectors (numerical representations).\n",
    "tagger: Assigns part-of-speech tags to words (e.g., noun, verb).\n",
    "parser: Analyzes sentence structure (syntax), building dependency trees.\n",
    "attribute_ruler: Modifies word attributes (like part-of-speech or lemma) based on rules.\n",
    "lemmatizer: Reduces words to their base or dictionary form (e.g., \"running\" to \"run\").\n",
    "ner: Identifies named entities, such as names of people, places, or organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processes the text and prints each token along with its part-of-speech tag and lemma (base form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony  |  PROPN  |  Tony\n",
      "gave  |  VERB  |  give\n",
      "two  |  NUM  |  two\n",
      "$  |  NUM  |  $\n",
      "to  |  ADP  |  to\n",
      "Peter  |  PROPN  |  Peter\n",
      ",  |  PUNCT  |  ,\n",
      "Bruce  |  PROPN  |  Bruce\n",
      "gave  |  VERB  |  give\n",
      "500  |  NUM  |  500\n",
      "€  |  NOUN  |  €\n",
      "to  |  PART  |  to\n",
      "Steve  |  PROPN  |  Steve\n"
     ]
    }
   ],
   "source": [
    "text = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token,\" | \", token.pos_,\" | \",token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifies and prints named entities in the text, along with their label (type) and a brief explanation of each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony  -  PERSON : People, including fictional\n",
      "2 $  -  MONEY : Monetary values, including unit\n",
      "Peter, Bruce gave  -  ORG : Companies, agencies, institutions, etc.\n",
      "500 €  -  MONEY : Monetary values, including unit\n",
      "Steve  -  PERSON : People, including fictional\n"
     ]
    }
   ],
   "source": [
    "text = \"Tony gave 2 $ to Peter, Bruce gave 500 € to Steve\"\n",
    "\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" - \" , ent.label_, \":\", spacy.explain(ent.label_))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the token in a fancier way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tony\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " gave \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2 $\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " to \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Peter, Bruce gave\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    500 €\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " to \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style= \"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORD EMBEDDING IN SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I  Has Vector: True  OOV:  False\n",
      "love  Has Vector: True  OOV:  False\n",
      "watching  Has Vector: True  OOV:  False\n",
      "football  Has Vector: True  OOV:  False\n",
      "and  Has Vector: True  OOV:  False\n",
      "acmilan  Has Vector: False  OOV:  True\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(\"I love watching football and acmilan\")\n",
    "\n",
    "for token in doc: # token.has_vector is a property in spaCy that checks whether the token (word) has a word vector associated with it.\n",
    "    print(token.text,\" Has Vector:\" ,token.has_vector, \" OOV: \",token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8733e-01,  4.0595e-01, -5.1174e-01, -5.5482e-01,  3.9716e-02,\n",
       "        1.2887e-01,  4.5137e-01, -5.9149e-01,  1.5591e-01,  1.5137e+00,\n",
       "       -8.7020e-01,  5.0672e-02,  1.5211e-01, -1.9183e-01,  1.1181e-01,\n",
       "        1.2131e-01, -2.7212e-01,  1.6203e+00, -2.4884e-01,  1.4060e-01,\n",
       "        3.3099e-01, -1.8061e-02,  1.5244e-01, -2.6943e-01, -2.7833e-01,\n",
       "       -5.2123e-02, -4.8149e-01, -5.1839e-01,  8.6262e-02,  3.0818e-02,\n",
       "       -2.1253e-01, -1.1378e-01, -2.2384e-01,  1.8262e-01, -3.4541e-01,\n",
       "        8.2611e-02,  1.0024e-01, -7.9550e-02, -8.1721e-01,  6.5621e-03,\n",
       "        8.0134e-02, -3.9976e-01, -6.3131e-02,  3.2260e-01, -3.1625e-02,\n",
       "        4.3056e-01, -2.7270e-01, -7.6020e-02,  1.0293e-01, -8.8653e-02,\n",
       "       -2.9087e-01, -4.7214e-02,  4.6036e-02, -1.7788e-02,  6.4990e-02,\n",
       "        8.8451e-02, -3.1574e-01, -5.8522e-01,  2.2295e-01, -5.2785e-02,\n",
       "       -5.5981e-01, -3.9580e-01, -7.9849e-02, -1.0933e-02, -4.1722e-02,\n",
       "       -5.5576e-01,  8.8707e-02,  1.3710e-01, -2.9873e-03, -2.6256e-02,\n",
       "        7.7330e-02,  3.9199e-01,  3.4507e-01, -8.0130e-02,  3.3451e-01,\n",
       "        2.7063e-01, -2.4544e-02,  7.2576e-02, -1.8120e-01,  2.3693e-01,\n",
       "        3.9977e-01,  4.5012e-01,  2.7179e-02,  2.7400e-01,  1.4791e-01,\n",
       "       -5.8324e-03,  9.5910e-01, -1.0129e+00,  2.0699e-01,  1.8237e-01,\n",
       "       -2.5234e-01, -2.6261e-01, -3.4799e-01, -2.4051e-02,  4.4470e-01,\n",
       "        5.9226e-02,  4.5561e-01,  1.9700e-01, -4.8327e-01,  8.9523e-02,\n",
       "       -2.2373e-01, -1.5654e-01,  2.1578e-01,  1.1673e-01,  8.2006e-02,\n",
       "       -8.0735e-01,  2.3903e-01, -5.1304e-01, -3.3888e-01, -3.1499e-01,\n",
       "       -1.7272e-01, -6.7020e-01,  2.7096e-01, -4.3241e-01,  4.3103e-02,\n",
       "        2.1233e-02,  1.3350e-02, -6.3938e-02, -2.4957e-01, -2.4938e-01,\n",
       "        3.4812e-01, -7.1321e-02,  2.3375e-01, -9.5384e-02,  5.2488e-01,\n",
       "        6.8175e-01, -1.0214e-01, -1.4914e-01, -7.5697e-02,  1.7248e-01,\n",
       "        2.5440e-01,  1.5760e-01, -5.9125e-01,  2.4300e-01,  6.3962e-01,\n",
       "       -9.3280e-02, -2.7914e-01, -6.6262e-02, -6.7170e-02, -4.0929e-01,\n",
       "       -3.0300e+00,  1.8250e-01,  2.0113e-01,  6.0628e-02, -2.4769e-01,\n",
       "        5.5324e-02, -4.9106e-01,  3.1544e-01, -3.4231e-01, -6.3766e-01,\n",
       "       -3.6129e-01, -5.9029e-02,  1.5510e-01,  4.4577e-02,  2.3572e-01,\n",
       "       -1.7095e-01, -2.2749e-01, -2.3184e-02,  2.3868e-01,  2.8170e-02,\n",
       "        4.2965e-01, -1.2458e-01, -3.6972e-02,  2.0061e-01, -3.1405e-01,\n",
       "       -8.5287e-02, -3.3496e-01, -9.7047e-02, -1.4388e-01,  1.1147e-01,\n",
       "       -4.5232e-01, -2.4217e-01, -1.8245e-01, -6.7292e-01,  2.1933e-02,\n",
       "       -5.4816e-02, -4.6508e-01,  4.7767e-01, -2.4752e-01, -1.5790e-01,\n",
       "        1.1817e-01,  5.6851e-02, -4.9151e-01,  1.5496e-01,  1.6425e-02,\n",
       "        4.1650e-02, -3.4990e-01, -1.5979e-01,  3.9705e-01,  2.2963e-01,\n",
       "        2.4688e-01,  1.9567e-02, -2.8802e-01, -6.9983e-01,  3.2744e-01,\n",
       "        1.0833e-01,  2.4945e-01, -7.8653e-01, -6.1379e-02, -3.7359e-01,\n",
       "       -1.1603e-01, -2.4950e-01,  1.0161e-01,  3.3994e-02,  1.5650e-01,\n",
       "        2.1344e-01, -1.1094e-01, -5.7687e-03,  1.7869e-01, -1.0127e-01,\n",
       "       -1.6891e-02,  3.0001e-01, -3.4116e-01, -3.2390e-02,  4.2514e-02,\n",
       "        1.1850e-01, -1.8337e-01, -6.2865e-01, -2.8021e-01,  4.2351e-01,\n",
       "        1.1277e-01,  1.2121e-03,  1.5710e-01, -3.6321e-01, -4.9251e-01,\n",
       "        1.1653e-01,  2.4024e-01,  1.7712e-01,  6.8700e-02, -4.4137e-01,\n",
       "       -2.9877e-01, -1.2071e-02,  2.8325e-01,  1.0668e-01, -1.8859e-01,\n",
       "       -4.1345e-01, -3.4090e-01,  4.7236e-02, -3.8309e-01,  4.3572e-01,\n",
       "        2.4505e-01,  2.7337e-01, -7.3038e-02,  4.2514e-01, -3.2455e-02,\n",
       "       -3.5211e-01,  4.5691e-01,  1.9433e-01, -1.5230e-01,  4.2675e-01,\n",
       "        2.8795e-01, -5.5969e-01, -1.3031e-01,  8.9844e-02,  4.2605e-01,\n",
       "       -1.9632e-01, -7.1989e-02, -8.0189e-02, -3.0425e-01, -4.6190e-01,\n",
       "        2.8178e-01, -9.9872e-02,  3.5097e-01,  1.6123e-01, -3.6548e-02,\n",
       "       -3.6739e-01, -1.9819e-02,  3.2130e-01,  1.7479e-01,  2.5175e-01,\n",
       "       -7.6439e-03, -9.3786e-02, -3.7852e-01,  4.3725e-01,  2.1288e-01,\n",
       "        2.5096e-01, -1.9613e-01, -2.8865e-01, -5.6726e-03,  4.2795e-01,\n",
       "        2.0625e-01, -3.7701e-02, -1.2200e-01, -7.9253e-02, -1.0290e-01,\n",
       "        1.0558e-02,  4.9880e-01,  2.5382e-01,  1.5526e-01,  1.7951e-03,\n",
       "        1.1633e-01,  7.9300e-02, -3.9142e-01, -3.2483e-01,  6.3451e-01,\n",
       "       -1.8910e-01,  5.4050e-02,  1.6495e-01,  1.8757e-01,  5.3874e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieves the word vector (numerical representation) of the first token in the document\n",
    "doc[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "doc[0].vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1 = nlp(\"bread\")\n",
    "token1[0].vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check similarity score betwween tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I  -  bread 0.26605986855569747\n",
      "ate  -  bread 0.5356433485040003\n",
      "a  -  bread 0.24572681149853734\n",
      "sandwich  -  bread 0.6874560014053445\n",
      "with  -  bread 0.30557175171805506\n",
      "french  -  bread 0.42025075623170777\n",
      "fries  -  bread 0.6145180843733152\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I ate a sandwich with french fries\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, \" - \", token1.text,token.similarity(token1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similarity(base_word , words_to_compare):\n",
    "    base_token = nlp(base_word)\n",
    "    doc = nlp(words_to_compare)\n",
    "    for token in doc:\n",
    "            print(token.text, \" - \", base_token.text,token.similarity(base_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple  -  iphone 0.6339781147910419\n",
      "samsung  -  iphone 0.6678678014329177\n",
      "iphone  -  iphone 1.0000000285783557\n",
      "dog  -  iphone 0.17431037640553934\n",
      "kitten  -  iphone 0.14685812907484028\n",
      "technology  -  iphone 0.23019987023206995\n"
     ]
    }
   ],
   "source": [
    "print_similarity(\"iphone\",\"apple samsung iphone dog kitten technology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78808445]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "king = nlp.vocab[\"king\"].vector\n",
    "man = nlp.vocab[\"man\"].vector\n",
    "woman = nlp.vocab[\"woman\"].vector\n",
    "queen = nlp.vocab[\"queen\"].vector\n",
    "\n",
    "result = king - man + woman\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity([result],[queen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can do lemmatization using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loving  |  love  |  3702023516439754181\n",
      "better  |  well  |  4525988469032889948\n",
      "did  |  do  |  2158845516055552166\n",
      "ability  |  ability  |  11565809527369121409\n",
      "encouter  |  encouter  |  14036664800987158468\n",
      "wiser  |  wise  |  1716070729102715479\n",
      "adjustable  |  adjustable  |  6033511944150694480\n",
      "based  |  base  |  4715552063986449646\n"
     ]
    }
   ],
   "source": [
    "doc = nlp (\"Loving better did ability encouter wiser adjustable based\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \" , token.lemma_, \" | \", token.lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the part of speech (POS) of each token in the text and their tags (a further categorization of parts of speech like the tense of a verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon  |  PROPN  :  proper noun  |  NNP  :  noun, proper singular\n",
      "flies  |  VERB  :  verb  |  VBZ  :  verb, 3rd person singular present\n",
      "to  |  ADP  :  adposition  |  IN  :  conjunction, subordinating or preposition\n",
      "mars  |  PROPN  :  proper noun  |  NNP  :  noun, proper singular\n",
      "yesterday  |  NOUN  :  noun  |  NN  :  noun, singular or mass\n",
      "He  |  PRON  :  pronoun  |  PRP  :  pronoun, personal\n",
      "carried  |  VERB  :  verb  |  VBD  :  verb, past tense\n",
      "pizza  |  NOUN  :  noun  |  NN  :  noun, singular or mass\n",
      "and  |  CCONJ  :  coordinating conjunction  |  CC  :  conjunction, coordinating\n",
      "pasta  |  NOUN  :  noun  |  NN  :  noun, singular or mass\n",
      "with  |  ADP  :  adposition  |  IN  :  conjunction, subordinating or preposition\n",
      "him  |  PRON  :  pronoun  |  PRP  :  pronoun, personal\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Elon flies to mars yesterday. He carried pizza and pasta with him\")\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ not in[\"SPACE\",\"X\",\"PUNCT\"]:\n",
    "        print(token, \" | \", token.pos_, \" : \",spacy.explain(token.pos_), \" | \",token.tag_, \" : \", spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use spaCy for NER (Named entity recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG : Companies, agencies, institutions, etc.\n",
      "Twitter  |  PERSON : People, including fictional\n",
      "45$ Billion  |  MONEY : Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire Twitter company for 45$ Billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \":\",spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Twitter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " company for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    45$ Billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc,style=\"ent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
